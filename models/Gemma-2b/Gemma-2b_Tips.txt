 Optimize for Raspberry Pi 5
Gemma 2B is too big for a Raspberry Pi without optimizations. To make it work:

- Use a quantized version (4-bit GGUF)
- Run it on llama.cpp or GGUF-compatible loaders
- Offload to a TPU/NPU accelerator if available
- Convert to GGUF (if needed)


python convert-to-gguf.py --input model.safetensors --output model.gguf